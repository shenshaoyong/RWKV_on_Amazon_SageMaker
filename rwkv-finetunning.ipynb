{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3e31c8-f918-46b6-b47c-1933f426cbae",
   "metadata": {},
   "source": [
    "# fine tuning rwkv using Huggingface Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e23643-9c67-4124-820e-4ce9483a96e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upgrade sdk library\n",
    "!pip install -qU sagemaker\n",
    "!pip install -qU boto3\n",
    "!pip install -qU botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b92a1-14bd-4146-8715-89b29aeece58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sagemaker environment setting\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import shutil\n",
    "import sagemaker.huggingface\n",
    "from sagemaker.djl_inference.model import DJLModel,DeepSpeedModel,HuggingFaceAccelerateModel,DJLPredictor\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sagemaker_session is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sagemaker_session = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf17889-af46-492d-970d-25c9b987e63e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_dir = 'source_dir'\n",
    "if not os.path.exists(source_dir):\n",
    "    os.mkdir(source_dir)\n",
    "#entry_point = 'entry_point.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05377f2-5944-41bb-a028-8988ae777082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $source_dir/requirements.txt\n",
    "transformers\n",
    "torch\n",
    "accelerate\n",
    "datasets\n",
    "numpy\n",
    "boto3\n",
    "sagemaker\n",
    "sentencepiece\n",
    "nvgpu==0.9.0\n",
    "pynvml==11.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19f058-9681-475e-bdb6-3086bfe5177f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $source_dir/finetune.py\n",
    "from transformers import (\n",
    "    RwkvForCausalLM,\n",
    "    RwkvConfig,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import collections\n",
    "from typing import Any, Dict\n",
    "import math\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def remove_url_from_text(text: str):\n",
    "    \"\"\"Remove square brackets around linked text and (_URL_0_) after\"\"\"\n",
    "    return re.sub(r\"\\[|\\]|\\(_URL_\\d+_\\)\", \"\", text)\n",
    "\n",
    "def tokenize_function(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Concatenate and tokenize the answers in flattened ELI5 data\"\"\"\n",
    "    concatenated = [remove_url_from_text(\" \".join(x)) for x in examples[\"answers.text\"]]\n",
    "    return tokenizer(concatenated)\n",
    "\n",
    "\n",
    "def chunk(examples: Dict[str, Any], chunk_size: int = 256) -> Dict[str, Any]:\n",
    "    \"\"\"Concatenate and chunk batches of data\"\"\"\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated[list(examples.keys())[0]])\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    return {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def set_labels(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Add a labels column to the dataset which is a copy of input_ids\"\"\"\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "\n",
    "MODEL_NAME = \"sgugger/rwkv-430M-pile\"\n",
    "DATASET = \"eli5\"\n",
    "CHUNK_SIZE = 128\n",
    "TEST_SPLIT_SIZE = 0.2\n",
    "BATCH_SIZE = 32\n",
    "DATASET_SPLIT = \"train_asks[:500]\"\n",
    "\n",
    "model = RwkvForCausalLM.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(DATASET, split=DATASET_SPLIT)\n",
    "dataset = dataset.train_test_split(test_size=TEST_SPLIT_SIZE)\n",
    "dataset = dataset.flatten()\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    # Encode\n",
    "encoded_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# Chunk\n",
    "chunked_dataset = encoded_dataset.map(\n",
    "    chunk,\n",
    "    fn_kwargs={\"chunk_size\": CHUNK_SIZE},\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Label\n",
    "lm_dataset = chunked_dataset.map(\n",
    "    set_labels,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = MODEL_NAME + \"-\" + DATASET,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=len(lm_dataset[\"train\"]) // BATCH_SIZE,\n",
    "    save_strategy = \"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Evaluate before train\n",
    "eval_0 = trainer.evaluate()\n",
    "perplexity_0 = math.exp(eval_0[\"eval_loss\"])\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "trainer.save_model('/opt/ml/model')\n",
    "\n",
    "# Evaluate after train\n",
    "eval_f = trainer.evaluate()\n",
    "perplexity_f = math.exp(eval_f[\"eval_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e636ae-d506-4752-b138-727192c2f6cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch, TrainingCompilerConfig\n",
    "\n",
    "# the original max batch size that can fit into GPU memory without compiler\n",
    "batch_size_native=12\n",
    "learning_rate_native=float('5e-5')\n",
    "\n",
    "# an updated max batch size that can fit into GPU memory with compiler\n",
    "batch_size=64\n",
    "\n",
    "# update learning rate\n",
    "learning_rate=learning_rate_native/batch_size_native*batch_size\n",
    "\n",
    "hyperparameters={\n",
    "    \"n_gpus\": 1,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate\n",
    "}\n",
    "\n",
    "pytorch_estimator=PyTorch(\n",
    "    entry_point='finetune.py',\n",
    "    source_dir=source_dir, \n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    framework_version='1.13.1',\n",
    "    py_version='py39',\n",
    "    #hyperparameters=hyperparameters,\n",
    "    #compiler_config=TrainingCompilerConfig(),\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False\n",
    ")\n",
    "\n",
    "pytorch_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7296d7-cb24-4083-843e-1cc5fc3e57ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
